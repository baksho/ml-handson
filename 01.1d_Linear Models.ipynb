{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c97b1183-e543-4c6e-bd3c-2b02b95b8bbc",
   "metadata": {},
   "source": [
    "## 1.1. Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67f5873-efff-4fbe-84b9-d2c4e7a9009b",
   "metadata": {},
   "source": [
    "### 1.1.10. Bayesian Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e469bc9f-de5c-4512-ad7e-2a1ff0168730",
   "metadata": {},
   "source": [
    "Bayesian regression techniques can be used to include regularization parameters in the estimation procedure: the regularization parameter is not set in a hard sense but tuned to the data at hand.\n",
    "\n",
    "This can be done by introducing [uninformative priors](https://en.wikipedia.org/wiki/Non-informative_prior#Uninformative_priors) over the hyper parameters of the model. The $\\ell_2$\n",
    " regularization used in [Ridge regression and classification](https://scikit-learn.org/stable/modules/linear_model.html#ridge-regression) is equivalent to finding a maximum a posteriori estimation under a Gaussian prior over the coefficients $w$ with precision $\\lambda^{-1}$. Instead of setting `lambda` manually, it is possible to treat it as a random variable to be estimated from the data.\n",
    "\n",
    "To obtain a fully probabilistic model, the output $y$ is assumed to be Gaussian distributed around $Xw$:\n",
    "\n",
    "$$p(y|X, w, \\alpha) = \\mathcal{N}(y|Xw, \\alpha^{-1})$$\n",
    "\n",
    "where $\\alpha$ is again treated as a random variable that is to be estimated from the data.\n",
    "\n",
    "The **advantages** of Bayesian Regression are:\n",
    "\n",
    "- It adapts to the data at hand.\n",
    "- It can be used to include regularization parameters in the estimation procedure.\n",
    "\n",
    "The **disadvantages** of Bayesian regression include:\n",
    "\n",
    "- Inference of the model can be time consuming."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6d52a5-e36c-4c11-ae9e-d55c03425406",
   "metadata": {},
   "source": [
    "$\\textbf{References}$\n",
    "\n",
    "- A good introduction to Bayesian methods is given in C. Bishop: Pattern Recognition and Machine learning\n",
    "- Original Algorithm is detailed in the book `Bayesian learning for neural networks` by Radford M. Neal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b0f990-49b6-4251-bb86-d9449670cb63",
   "metadata": {},
   "source": [
    "#### 1.1.10.1. Bayesian Ridge Regression\n",
    "\n",
    "[BayesianRidge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.BayesianRidge.html#sklearn.linear_model.BayesianRidge) estimates a probabilistic model of the regression problem as described above. The prior for the coefficient $w$ is given by a spherical Gaussian:\n",
    "\n",
    "$$p(w|\\lambda) = \\mathcal{N}(w|0, \\lambda^{-1}I_p)$$\n",
    "\n",
    "The priors over $\\alpha$ and $\\lambda$ are chosen to be [gamma distributions](https://en.wikipedia.org/wiki/Gamma_distribution), the conjugate prior for the precision of the Gaussian. The resulting model is called *Bayesian Ridge Regression*, and is similar to the classical [Ridge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038a0f15-73f7-47c0-99a4-96100e175b7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
