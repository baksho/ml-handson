{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19c29d13-b7bb-4d55-9bb6-79938f7af157",
   "metadata": {},
   "source": [
    "## 1.1. Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837e05eb-c66c-4417-aecd-b4287613f73d",
   "metadata": {},
   "source": [
    "The following are a set of methods intended for regression in which the target value is expected to be a linear combination of the features. In mathematical notation, if $\\hat{y}$ is the predicted value.\n",
    "\n",
    "$$\\hat{y}(w, x) = w_0 + w_1x_1 + \\dots + w_px_p$$\n",
    "\n",
    "Across the module, we designate the vector $w = (w_1, \\dots, w_p)$ as `coef_` and $w_0$ as `intercept_`.\n",
    "\n",
    "To perform classification with generalized linear models, see **Logistic Regression**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3803fe-a3a7-4b9b-b90c-73775986038a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.1.1. Ordinary Least Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c63d08c-54d7-47e2-8611-f44247b292cc",
   "metadata": {},
   "source": [
    "[LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression) fits a linear model with coefficients $w = (w_1, \\dots, w_p)$ to minimize the residual sum of squares between the observed targets in the dataset, and the targets predicted by the linear approximation. Mathematically it solves a problem of the form:\n",
    "\n",
    "$$\\min_w \\lVert Xw - y \\rVert_2^2$$\n",
    "<center><img src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_ols_001.png\" /></center>\n",
    "\n",
    "[LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression) will take in its `fit` method arrays `X`, `y` and will store the coefficients $w$ of the linear model in its `coef_` member:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7c7f4b4-add8-4021-94a2-e31112ac029d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.5, 0.5])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "reg = linear_model.LinearRegression()\n",
    "reg.fit([[0, 0], [1, 1], [2, 2]], [0, 1, 2])\n",
    "reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35844b50-c336-46fb-b6c6-9f5187b330dd",
   "metadata": {},
   "source": [
    "The coefficient estimates for Ordinary Least Squares rely on the independence of the features. When features are correlated and the columns of the design matrix $X$ have an approximately linear dependence, the design matrix becomes close to singular and as a result, the least-squares estimate becomes highly sensitive to random errors in the observed target, producing a large variance. This situation of _**multicollinearity**_ can arise, for example, when data are collected without an experimental design."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b89a72c-64ae-4957-8fe0-4bd13e2a9ed4",
   "metadata": {},
   "source": [
    "#### 1.1.1.1. Non-Negative Least Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2555c2-855c-4411-b8fb-24e55cf41e3f",
   "metadata": {},
   "source": [
    "It is possible to constrain all the coefficients to be non-negative, which may be useful when they represent some physical or naturally non-negative quantities (e.g., frequency counts or prices of goods).\n",
    "[LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression) accepts a boolean `positive` parameter: when set to `True`, [Non-Negative Least Squares](https://en.wikipedia.org/wiki/Non-negative_least_squares) are then applied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8942c8e5-1446-41ec-9344-531f2178cd9b",
   "metadata": {},
   "source": [
    "In mathematical optimiyation, the problem of **non-negative least squares (NNLS)** is a type of constrained least squares problem where the coefficients are not allowed to become negative. That is, given a matrix $A$ and a (column) vector of response variables $y$, the goal is to find:\n",
    "\n",
    "$$\\text{arg}\\min_x \\lVert Ax - y\\rVert_2^2$$ subject to $$x \\geq 0$$\n",
    "\n",
    "Here $x \\geq 0$ means that each component of the vector $x$ should be non-negative, and $\\lVert \\cdot \\rVert_2$ denotes the Euclidean norm.\n",
    "\n",
    "Non-negative least squares problems turn up as subproblems in matrix decomposition, e.g. in algorithms for PARAFAC and non-negative matrix/tensor factorization. The latter can be considered a generalization of NNLS.\n",
    "\n",
    "Another generalization of NNLS is **bounded-variable least squares** (BVLS), with simultaneous upper and lower bounds $\\alpha_i \\leq x_i \\leq \\beta_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8000caf5-948f-4438-856b-fc895eaff13f",
   "metadata": {},
   "source": [
    "**Quadratic programming version**\n",
    "\n",
    "The NNLS problem is equivalent to a quadratic programming problem:\n",
    "\n",
    "$$\\text{arg}\\min_{x \\geq 0} (\\frac{1}{2}x^TQx+c^Tx)$$\n",
    "\n",
    "where $Q = A^TA$ and $c = -A^Ty$. This problem is convex, as $Q$ is positive semidefinite and the non-negativity constraints form a convex feasible set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57f3cf7-5e62-4b5f-ad40-485b2d1286ce",
   "metadata": {},
   "source": [
    "#### 1.1.1.2. Ordinary Least Squares Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ad742d-5e29-4c9c-956b-6e9c1b6e8cf8",
   "metadata": {},
   "source": [
    "The least squares solution is computed using the singular value decomposition of $X$. If $X$ is a matrix of shape `(n_samples, n_features)`, this method has a cost of $O(n_{samples}n^2_{features})$, assuming that $n_{samples} \\geq n_{features}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ae4149-422b-4fec-ae7b-b9f4c5646b09",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.1.2. Ridge Regression and Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982445b8-b65d-465e-b110-438451ea1716",
   "metadata": {},
   "source": [
    "#### 1.1.2.1. Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bf9cbb-7387-4ae2-a21d-e449eb88a22e",
   "metadata": {},
   "source": [
    "[Ridge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge) regression addresses some of the problems of [Ordinary Least Squares](https://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares) by imposing a penalty on the size of the coefficients. The ridge coefficients minimize a penalized residual sum of squares:\n",
    "\n",
    "$$\\min_w \\lVert Xw - y \\rVert_2^2 + \\alpha \\lVert Xw - y \\rVert_2^2$$\n",
    "\n",
    "The complexity parameter $\\alpha \\geq 0$ controls the amount of shrinkage: the larger the value of $\\alpha$, the greater the amount of shrinkage and thus the coefficients become more robust to collinearity.\n",
    "<center><img src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_ridge_path_001.png\" /></center>\n",
    "\n",
    "As with other linear models, [Ridge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge) will take in its fit method arrays X, y and will store the coefficients `fit` method arrays `X`, `y` and will store the coefficients $w$ of the linear model in its `coef_` member:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe5acdf8-af1e-4e40-8cd7-96bfae32abdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1363636363636364"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "reg = linear_model.Ridge(alpha=.5)\n",
    "reg.fit([[0, 0], [0, 0], [1, 1]], [0, .1, 1])\n",
    "reg.coef_\n",
    "reg.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7aee95-cfdd-401b-8847-90883127afdf",
   "metadata": {},
   "source": [
    "Note that the class  [Ridge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge) allows for the user to specify that the solver be automatically chosen by setting `solver=\"auto\"`. When this option is specified,  [Ridge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge) will choose between the `\"lbfgs\"`, `\"cholesky\"`, and `\"sparse_cg\"` solvers.  [Ridge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge) will begin checking the conditions shown in the following table from top to bottom. If the condition is true, the corresponding solver is chosen.\n",
    "\n",
    "| Solver      | Condition                                  |\n",
    "| ----------- | -------------------------------------------|\n",
    "| 'lbfgs'     | The `positive=True` option is specified.   |\n",
    "| 'cholesky'  | The input array X is not sparse.           |\n",
    "| 'sparse_cg' | None of the above conditions are fulfilled.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6195b48-26a9-4837-a1ef-934d95213d32",
   "metadata": {},
   "source": [
    "#### 1.1.2.2. Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe34c3e8-a3b2-4c61-abf5-323d4352a765",
   "metadata": {},
   "source": [
    "The [Ridge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge) regressor has a classifier variant: [RidgeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeClassifier.html#sklearn.linear_model.RidgeClassifier). This classifier first converts binary targets to `{-1, 1}` and then treats the problem as a regression task, optimizing the same objective as above. The predicted class corresponds to the sign of the regressorâ€™s prediction. For multiclass classification, the problem is treated as multi-output regression, and the predicted class corresponds to the output with the highest value.\n",
    "\n",
    "It might seem questionable to use a (penalized) Least Squares loss to fit a classification model instead of the more traditional logistic or hinge losses. However, in practice, all those models can lead to similar cross-validation scores in terms of accuracy or precision/recall, while the penalized least squares loss used by the [RidgeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeClassifier.html#sklearn.linear_model.RidgeClassifier) allows for a very different choice of the numerical solvers with distinct computational performance profiles.\n",
    "\n",
    "The [RidgeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeClassifier.html#sklearn.linear_model.RidgeClassifier) can be significantly faster than e.g. [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression) with a high number of classes because it can compute the projection matrix $(X^TX)^{-1}X^T$ only once.\n",
    "\n",
    "This classifier is sometimes referred to as a [Least Squares Support Vector Machines](https://en.wikipedia.org/wiki/Least-squares_support_vector_machine) with a linear kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26034311-c6dc-4f67-b775-8c965bce230e",
   "metadata": {},
   "source": [
    "**Least-squares Support Vector Machines (LS-SVM)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa954457-bc2b-4f9f-aab8-b9dbb0241913",
   "metadata": {},
   "source": [
    "Least-squares support-vector machines (LS-SVM) for statistics and in statistical modeling, are least-squares versions of support-vector machines (SVM), which are a set of related supervised learning methods that analyze data and recognize patterns, and which are used for classification and regression analysis.\n",
    "\n",
    "In this version one finds the solution by solving a set of linear equations instead of a convex quadratic programming (QP) problem for classical SVMs. LS-SVMs are a class of kernel-based learning methods.\n",
    "\n",
    "Least-squares SVM classifiers were proposed by Johan Suykens and Joos Vandewalle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1df09b-9824-43dd-886b-2c051f1a129b",
   "metadata": {},
   "source": [
    "#### 1.1.2.3. Ridge Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6cc564-772b-4789-a3fb-59aaab8c4c60",
   "metadata": {},
   "source": [
    "This method has the same order of complexity as [Ordinary Least Squares](https://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d458f396-5820-459e-9abd-aaa1f2daf4e9",
   "metadata": {},
   "source": [
    "#### 1.1.2.4. Setting the regularization parameter: leave-one-out Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6062e42-6c1a-40a6-93e2-871d7cec964e",
   "metadata": {},
   "source": [
    "RidgeCV and RidgeClassifierCV implement ridge regression/classification with built-in cross-validation of the alpha parameter. They work in the same way as GridSearchCV except that it defaults to efficient Leave-One-Out cross-validation. When using the default cross-validation, alpha cannot be 0 due to the formulation used to calculate Leave-One-Out error. See [RL2007] for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0ef644-9c0f-4ae0-9aaf-7d21d630b87b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
