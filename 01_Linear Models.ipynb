{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19c29d13-b7bb-4d55-9bb6-79938f7af157",
   "metadata": {},
   "source": [
    "## 1.1. Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837e05eb-c66c-4417-aecd-b4287613f73d",
   "metadata": {},
   "source": [
    "The following are a set of methods intended for regression in which the target value is expected to be a linear combination of the features. In mathematical notation, if $\\hat{y}$ is the predicted value.\n",
    "\n",
    "$$\\hat{y}(w, x) = w_0 + w_1x_1 + \\dots + w_px_p$$\n",
    "\n",
    "Across the module, we designate the vector $w = (w_1, \\dots, w_p)$ as `coef_` and $w_0$ as `intercept_`.\n",
    "\n",
    "To perform classification with generalized linear models, see **Logistic Regression**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3803fe-a3a7-4b9b-b90c-73775986038a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.1.1. Ordinary Least Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c63d08c-54d7-47e2-8611-f44247b292cc",
   "metadata": {},
   "source": [
    "[LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression) fits a linear model with coefficients $w = (w_1, \\dots, w_p)$ to minimize the residual sum of squares between the observed targets in the dataset, and the targets predicted by the linear approximation. Mathematically it solves a problem of the form:\n",
    "\n",
    "$$\\min_w \\lVert Xw - y \\rVert_2^2$$\n",
    "<center><img src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_ols_001.png\"/></center>\n",
    "\n",
    "[LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression) will take in its `fit` method arrays `X`, `y` and will store the coefficients $w$ of the linear model in its `coef_` member:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7c7f4b4-add8-4021-94a2-e31112ac029d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.5, 0.5])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "reg = linear_model.LinearRegression()\n",
    "reg.fit([[0, 0], [1, 1], [2, 2]], [0, 1, 2])\n",
    "reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35844b50-c336-46fb-b6c6-9f5187b330dd",
   "metadata": {},
   "source": [
    "The coefficient estimates for Ordinary Least Squares rely on the independence of the features. When features are correlated and the columns of the design matrix $X$ have an approximately linear dependence, the design matrix becomes close to singular and as a result, the least-squares estimate becomes highly sensitive to random errors in the observed target, producing a large variance. This situation of _**multicollinearity**_ can arise, for example, when data are collected without an experimental design."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b89a72c-64ae-4957-8fe0-4bd13e2a9ed4",
   "metadata": {},
   "source": [
    "#### 1.1.1.1. Non-Negative Least Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2555c2-855c-4411-b8fb-24e55cf41e3f",
   "metadata": {},
   "source": [
    "It is possible to constrain all the coefficients to be non-negative, which may be useful when they represent some physical or naturally non-negative quantities (e.g., frequency counts or prices of goods).\n",
    "[LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression) accepts a boolean `positive` parameter: when set to `True`, [Non-Negative Least Squares](https://en.wikipedia.org/wiki/Non-negative_least_squares) are then applied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8942c8e5-1446-41ec-9344-531f2178cd9b",
   "metadata": {},
   "source": [
    "In mathematical optimiyation, the problem of **non-negative least squares (NNLS)** is a type of constrained least squares problem where the coefficients are not allowed to become negative. That is, given a matrix $A$ and a (column) vector of response variables $y$, the goal is to find:\n",
    "\n",
    "$$\\text{arg}\\min_x \\lVert Ax - y\\rVert_2^2$$ subject to $$x \\geq 0$$\n",
    "\n",
    "Here $x \\geq 0$ means that each component of the vector $x$ should be non-negative, and $\\lVert \\cdot \\rVert_2$ denotes the Euclidean norm.\n",
    "\n",
    "Non-negative least squares problems turn up as subproblems in matrix decomposition, e.g. in algorithms for PARAFAC and non-negative matrix/tensor factorization. The latter can be considered a generalization of NNLS.\n",
    "\n",
    "Another generalization of NNLS is **bounded-variable least squares** (BVLS), with simultaneous upper and lower bounds $\\alpha_i \\leq x_i \\leq \\beta_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8000caf5-948f-4438-856b-fc895eaff13f",
   "metadata": {},
   "source": [
    "**Quadratic programming version**\n",
    "\n",
    "The NNLS problem is equivalent to a quadratic programming problem:\n",
    "\n",
    "$$\\text{arg}\\min_{x \\geq 0} (\\frac{1}{2}x^TQx+c^Tx)$$\n",
    "\n",
    "where $Q = A^TA$ and $c = -A^Ty$. This problem is convex, as $Q$ is positive semidefinite and the non-negativity constraints form a convex feasible set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57f3cf7-5e62-4b5f-ad40-485b2d1286ce",
   "metadata": {},
   "source": [
    "#### 1.1.1.2. Ordinary Least Squares Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ad742d-5e29-4c9c-956b-6e9c1b6e8cf8",
   "metadata": {},
   "source": [
    "The least squares solution is computed using the singular value decomposition of $X$. If $X$ is a matrix of shape `(n_samples, n_features)`, this method has a cost of $O(n_{samples}n^2_{features})$, assuming that $n_{samples} \\geq n_{features}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ae4149-422b-4fec-ae7b-b9f4c5646b09",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.1.2. Ridge Regression and Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327767d8-3af7-44a4-9d60-01a5b04ae097",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
